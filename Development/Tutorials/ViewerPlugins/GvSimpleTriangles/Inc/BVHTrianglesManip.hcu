/*
 * GigaVoxels - GigaSpace
 *
 * Website: http://gigavoxels.inrialpes.fr/
 *
 * Contributors: GigaVoxels Team
 *
 * BSD 3-Clause License:
 *
 * Copyright (C) 2007-2015 INRIA - LJK (CNRS - Grenoble University), All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the organization nor the names  of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL COPYRIGHT HOLDER BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

//BVHTrianglesManip.hcu
#ifndef BVHTrianglesManip_hcu
#define BVHTrianglesManip_hcu

// Gigavoxels
//#include <GvCore/IntersectionTests.hcu>
#include <GvRendering/GvRendererHelpersKernel.h>

//#include "RendererHelpers.hcu"
//
//#include "GPUTreeBVH.hcu"
//#include "IntersectionTests.h"

/******************************************************************************
 * ...
 ******************************************************************************/
__device__
void groupParallelSum( uint Tid, int* data, uint n )
{
	for ( uint stride = n >> 1; stride > 0; stride = stride >> 1 )
	{
		// Thread Synchronization
		__syncthreads();

		if ( Tid < stride )
		{
			data[ Tid ] += data[ Tid + stride ];
		}
	}
}

/******************************************************************************
 * ...
 ******************************************************************************/
__device__
void groupParallelSum( uint Tid, float* data, uint n )
{
	for ( uint stride = n >> 1; stride > 0; stride = stride >> 1 )
	{
		// Thread Synchronization
		__syncthreads();

		if ( Tid < stride )
		{
			data[ Tid ] += data[ Tid + stride ];
		}
	}
}

/******************************************************************************
 * ...
 ******************************************************************************/
__device__
void groupParallelMin( uint Tid, float* data, uint n )
{
	for ( uint stride = n >> 1; stride > 0; stride = stride >> 1 )
	{
		if ( Tid < stride )
		{
			data[ Tid ] = data[ Tid ] <= data[ Tid + stride ] ? data[ Tid ] : data[ Tid + stride ];
			//data[Tid]=fminf(data[Tid], data[Tid+stride]);
		}

		// Thread Synchronization
		__syncthreads();
	}
}

/******************************************************************************
 * ...
 ******************************************************************************/
__device__
inline void groupParallelOR( uint Tid, uint* data, uint n )
{
	for ( uint stride = n >> 1; stride > 0; stride = stride >> 1 )
	{
		// Thread Synchronization
		__syncthreads();

		if ( Tid < stride )
		{
			data[ Tid ] = data[ Tid ] | data[ Tid + stride ];
		}
	}
}

/******************************************************************************
 ****************************** CLASS DEFINITION ******************************
 ******************************************************************************/

/** 
 * @class TraversalStack
 *
 * @brief The TraversalStack class provides stack management for elements
 *
 * @param T type of elements managed by the stack
 */
/** BVH traversal using shared stack. 'Realtime Ray Tracing on GPU with BVH-based Packet Traversal' */
template< class T >
class TraversalStack
{
	
	/**************************************************************************
	 ***************************** PUBLIC SECTION *****************************
	 **************************************************************************/

public:

	/****************************** INNER TYPES *******************************/

	/******************************* ATTRIBUTES *******************************/

	/**
	 * Current element index
	 */
	int curIndex;

	/**
	 * Array of elements managed by the stack
	 */
	T data[ BVH_TRAVERSAL_STACK_SIZE ];

	/******************************** METHODS *********************************/

	/**
	 * Initialize the stack
	 *
	 * @param Pid ...
	 */
	__device__
	void init( uint Pid )
	{
		// Ask only thread 0
		if ( Pid == 0 )
		{
			curIndex = 0;
		}

		// Thread Synchronization
		__syncthreads();
	}

	/**
	 * Return last element (top)
	 *
	 * @return ...
	 */
	__device__
	inline T& top()
	{
		return data[ curIndex ];
	}

	/**
	 * ...
	 */
	__device__
	inline T pop( uint Pid )
	{
		if ( Pid == 0 && curIndex > 0 )
		{
			curIndex--;
		}

		//__syncthreads();

		return data[ curIndex ];
	}

	/**
	 * ...
	 */
	__device__
	inline T pop()
	{
		//if ( curIndex > 0 )
			curIndex--;
		
		return data[ curIndex ];
	}

	/**
	 * Push an element in the stack
	 *
	 * @param Pid Index of thread in block to be able to ask only for one thread (index 0)
	 * @param val the element value
	 */
	__device__
	inline void push( uint Pid, const T& val )
	{
		if ( Pid == 0 )
		{
			if ( curIndex < BVH_TRAVERSAL_STACK_SIZE && curIndex >= 0 )
			{
				data[ curIndex ] = val;
				curIndex++;
			}
		}
	}

	/**
	 * Push an element in the stack
	 *
	 * @param val the element value
	 */
	__device__
	inline void push( const T& val )
	{
		//if ( curIndex < BVH_TRAVERSAL_STACK_SIZE && curIndex >= 0 ) {
			data[ curIndex ] = val;
			curIndex++;
		//}
	}

	/**
	 * Tell wheter or not the stack is empty
	 *
	 * @return a flag telling wheter or not the stack is empty
	 */
	__device__
	inline bool isEmpty()
	{
		//__syncthreads();

		return ( curIndex <= 0 );
	}

	/**
	 * Tell wheter or not the stack is empty
	 *
	 * @return a flag telling wheter or not the stack is empty
	 */
	__device__
	inline bool isFull()
	{
		//__syncthreads();

		return ( curIndex >= BVH_TRAVERSAL_STACK_SIZE );
	}

	/**************************************************************************
	 **************************** PROTECTED SECTION ***************************
	 **************************************************************************/

protected:

	/****************************** INNER TYPES *******************************/

	/******************************* ATTRIBUTES *******************************/

	/******************************** METHODS *********************************/

	/**************************************************************************
	 ***************************** PRIVATE SECTION ****************************
	 **************************************************************************/

private:

	/****************************** INNER TYPES *******************************/

	/******************************* ATTRIBUTES *******************************/

	/******************************** METHODS *********************************/

};

/** 
 * @struct CudaQueue
 *
 * @brief The CudaQueue struct provides ...
 *
 * @param T ...
 * @param Size ...
 */
template< class T, uint Size >
struct CudaQueue
{

	T data[ Size ];

	__device__
	inline void clear( T v )
	{
		switch ( Size )
		{
			case 8:
				data[7]=v;
			case 7:
				data[6]=v;
			case 6:
				data[5]=v;
			case 5:
				data[4]=v;
			case 4:
				data[3]=v;
			case 3:
				data[2]=v;
			case 2:
				data[1]=v;
			case 1:
				data[0]=v;
		}
	}


	__device__
	inline T& top()
	{
		return data[ 0 ];
	}

	__device__
	inline void push(const T& val )
	{
		switch ( Size )
		{
			case 8:
				data[7]=data[6];
			case 7:
				data[6]=data[5];
			case 6:
				data[5]=data[4];
			case 5:
				data[4]=data[3];
			case 4:
				data[3]=data[2];
			case 3:
				data[2]=data[1];
			case 2:
				data[1]=data[0];
		}

		data[0] = val;
	}
};

/**
 * ...
 */
__shared__ TraversalStack< uint > S;

/**
 * ...
 */
__shared__ TraversalStack< VolTreeBVHNodeUser > nodeStack;

/**
 * ...
 */
#define STACKALL_MODE 0

/**
 * ...
 */
template< class GPUTreeBVHType >
__device__
void d_bvhTraversalStep_SharedStack(GPUTreeBVHType &gpuTreeBVH, uint &Pid, int &maskedAt, uint2 &tileCoords, float3 &rayStart, float3 &rayDir, uint &Np, VolTreeBVHNodeUser &snode, float &t)
{
	//__shared__ VolTreeBVHNodeUser snode;
	__shared__ bool stopTraversal;

	if ( S.isEmpty() )
	{
		maskedAt=-1;
	}

	if ( Pid == 0 )
	{
		Np=S.pop();
		snode=nodeStack.pop();
	}
	__syncthreads();


	if ( Pid == 0 )
	{
		stopTraversal=true;
	}
	__syncthreads();

	if ( maskedAt != -1 && snode.hasSubNodes() )
	{
		stopTraversal=false;
	}
	__syncthreads();

	while ( ! stopTraversal )
	{
		if ( maskedAt > S.curIndex )
		{
			maskedAt = BVH_TRAVERSAL_STACK_SIZE;

			//Intersect ray with BVH node
			float interMin; float interMax;
			int objboxinter=intersectBox( rayStart, rayDir,  snode.bbMin(), snode.bbMax(), interMin, interMax);
			bool snodeInter= (objboxinter && interMax>0.0f);
			interMin=maxcc(interMin, 0.0f);

			if ( ! snodeInter )
			{
				maskedAt = S.curIndex;
			}
			else if ( interMin > t )
			{
				maskedAt = S.curIndex;
			}
		}
		__syncthreads();
		
		__shared__ VolTreeBVHNodeUser Nlr[ 2 ];
		__shared__ uint sharedHits[ 4 ];
		__shared__ float M[ NUM_RAYS_PER_BLOCK_X * NUM_RAYS_PER_BLOCK_Y ]; //BVH_NUM_THREADS_PER_BLOCK_X*BVH_NUM_THREADS_PER_BLOCK_Y

		uint b1, b2;
		float lambda1; float lambda2;
		float mu1; float mu2;

		//Parallel fetch of the 2 childs
		gpuTreeBVH.parallelFetchBVHNode( Pid, Nlr[ 0 ], snode.getSubNodeIdx() );
		//__syncthreads();
		gpuTreeBVH.parallelFetchBVHNode( Pid, Nlr[ 1 ], snode.getSubNodeIdx() + 1 );
		//__syncthreads();

		int objboxinter;
		//Intersect ray with BVH node
		bool hitobjectLambda;
		bool hitobjectMu;

		if ( maskedAt>S.curIndex  /*&& maskedAt!=-1*/)
		{
			objboxinter=intersectBox( rayStart, rayDir,  Nlr[0].bbMin(), Nlr[0].bbMax(), lambda1, lambda2);
			hitobjectLambda=objboxinter && lambda2>0.0f;
			lambda1=maxcc(lambda1, 0.0f);


			objboxinter=intersectBox( rayStart, rayDir,  Nlr[1].bbMin(), Nlr[1].bbMax(), mu1, mu2);
			hitobjectMu=objboxinter && mu2>0.0f;
			mu1=maxcc(mu1, 0.0f);
		}
		else
		{
			hitobjectLambda=false;
			hitobjectMu=false;
		}


		b1 =(uint)(hitobjectLambda );
		b2 =(uint)(hitobjectMu );

		if ( Pid < 4 )
			sharedHits[Pid]=0;
		__syncthreads();

		sharedHits[2*b1+b2]=true;
		__syncthreads();

		if ( sharedHits[3] || (sharedHits[1] && sharedHits[2] ) )
		{
			/*if(Pid==0)
				M[0]=-1;*/
			float b2state= (b2 && mu1<lambda1) ? 1.0f : 0.0f;

			if(!b1 && !b2)
				b2state=0.5f;

			M[Pid]=2.0f* b2state -1.0f;

			__syncthreads();

			groupParallelSum( Pid, M, NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y /*BVH_NUM_THREADS_PER_BLOCK_X*BVH_NUM_THREADS_PER_BLOCK_Y*/);
			/*if(Pid==0){
				float summ=0.0f;

				for(uint ii=0; ii<NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y; ii++){
					summ+=M[ii];
				}
				M[0]=summ;
			}*/
			__syncthreads();

			if ( M[0] < 0.0f )
			{
				/*if(maskedAt>S.curIndex && !b2)
					maskedAt=S.curIndex;*/
				if(Pid==0){
					S.push(snode.getSubNodeIdx()+1);
					nodeStack.push(Nlr[1]);
				}
				__syncthreads();

				/*if(maskedAt>S.curIndex && !b1)
					maskedAt=S.curIndex;*/
				if(Pid==0)
				{
#if STACKALL_MODE
					S.push(snode.getSubNodeIdx());
					nodeStack.push(Nlr[0]);
#else
					Np=snode.getSubNodeIdx();
					snode=Nlr[0];
#endif
				}
				//__syncthreads();


			}
			else
			{
				/*if(maskedAt>S.curIndex && !b1)
					maskedAt=S.curIndex;*/
				if ( Pid == 0 )
				{
					S.push(snode.getSubNodeIdx());
					nodeStack.push(Nlr[0]);
				}
				__syncthreads();
				/*if(maskedAt>S.curIndex && !b2)
					maskedAt=S.curIndex;*/
				if ( Pid == 0 )
				{
#if STACKALL_MODE
					S.push(snode.getSubNodeIdx()+1);
					nodeStack.push(Nlr[1]);
#else
					Np=snode.getSubNodeIdx()+1;
					snode=Nlr[1];
#endif
				}
				//__syncthreads();
			}

			//__syncthreads();

		}else{

			if(sharedHits[2]){ //Warning, M[2] and M[1] cases inverted in the original paper algorithm
				if(maskedAt>S.curIndex && !b1		  /*&& maskedAt!=-1*/)
						maskedAt=S.curIndex;

				if(Pid==0){
#if STACKALL_MODE
					S.push(snode.getSubNodeIdx());
					nodeStack.push(Nlr[0]);
#else
					Np=snode.getSubNodeIdx();
					snode=Nlr[0];
#endif
				}
				//__syncthreads();
			}else if(sharedHits[1]){

				if(maskedAt>S.curIndex && !b2	 /*&& maskedAt!=-1*/)
					maskedAt=S.curIndex;

				if(Pid==0){
#if STACKALL_MODE
					S.push(snode.getSubNodeIdx()+1);
					nodeStack.push(Nlr[1]);
#else
					Np=snode.getSubNodeIdx()+1;
					snode=Nlr[1];
#endif
				}
				//__syncthreads();
			}
#if STACKALL_MODE==0
			else{

				if(S.isEmpty()){
					maskedAt=-1;
				}else{
					if(Pid==0){
						Np=S.pop();
						snode=nodeStack.pop();
					}
				}
				//__syncthreads();
			}
#endif

		}

		__syncthreads();

#if STACKALL_MODE
		if(S.isEmpty()){
			maskedAt=-1;
		}

		__syncthreads();
		if(Pid==0){
			Np=S.pop();
			snode=nodeStack.pop();
		}
		__syncthreads();
#endif
		if(Pid==0)
			stopTraversal=true;
		__syncthreads();

		if( maskedAt!=-1 && snode.hasSubNodes() ){
			stopTraversal=false;
		}
		__syncthreads();

	}
}


/**
 * ...
 */
#if BVH_TRAVERSAL_USE_MASKSTACK
__shared__ TraversalStack<uint> nodeMaskStack;
#endif

/******************************************************************************
 * ...
 *
 * @param ... ...
 ******************************************************************************/
template<class GPUTreeBVHType>
__device__
void d_bvhTraversalStep_SharedStack2( GPUTreeBVHType& gpuTreeBVH, uint& Pid, int& maskedAt, uint2& tileCoords, float3& rayStart, float3& rayDir, uint& Np, VolTreeBVHNodeUser& snode, float& t, float& lastInterT, float& lastInterTmax )
{
	//__shared__ VolTreeBVHNodeUser snode;
	__shared__ bool stopTraversal;

	stopTraversal = false;
	
	while ( !stopTraversal )
	{
		if ( Np == 0 )
		{
			if ( Pid == 0 )
			{
				Np = S.pop();
				snode = nodeStack.pop();
			}

#if BVH_TRAVERSAL_USE_MASKSTACK
			__shared__ uint curBlockMask;
			if(Pid==0){
				curBlockMask=nodeMaskStack.pop();
			}
			__syncthreads();

			if(maskedAt>S.curIndex ){
				if(curBlockMask & (1<<Pid)){
					maskedAt=BVH_TRAVERSAL_STACK_SIZE;
				}else{
					maskedAt=S.curIndex;
				}
			}
#endif
			__syncthreads();

#if BVH_TRAVERSAL_USE_MASKSTACK==0
			if ( maskedAt > S.curIndex )
			{
				//Intersect ray with BVH node
				float interMin = 0.0f; float interMax = 1000.0f;
				int objboxinter = GvRendering::intersectBox( rayStart, rayDir,  snode.bbMin(), snode.bbMax(), interMin, interMax);
				bool snodeInter = (objboxinter && interMax>0.0f);
				//interMin=maxcc(interMin, 0.0f);

				lastInterT = interMin;
				lastInterTmax = interMax;

				float pixelSize = k_renderViewContext.pixelSize.x * ( interMin * 1.333f ) * k_renderViewContext.frustumNearINV * 1.0f;

				if ( !snodeInter || interMin>t || snode.bbox.maxSize() < pixelSize )
				{
					maskedAt = S.curIndex;
				}
				else
				{
					maskedAt = BVH_TRAVERSAL_STACK_SIZE;
				}
			}
#endif
			if ( !snode.hasSubNodes() )
			{
				break;
			}
		} //if(Np==0)

		__shared__ VolTreeBVHNodeUser Nlr[2];
		__shared__ uint sharedHits[4];
		__shared__ float M[ NUM_RAYS_PER_BLOCK_X * NUM_RAYS_PER_BLOCK_Y ];

		//Parallel fetch of the 2 childs
#if 0
		gpuTreeBVH.parallelFetchBVHNode(Pid, Nlr[0], snode.getSubNodeIdx());
		//__syncthreads();
		gpuTreeBVH.parallelFetchBVHNode(Pid, Nlr[1], snode.getSubNodeIdx()+1);
		//__syncthreads();
#else
		gpuTreeBVH.parallelFetchBVHNodeTile( Pid, Nlr, snode.getSubNodeIdx() );
#endif
		float interTNear[2]; float interTFar[2];
		interTNear[0] = interTNear[1] = 0.0f; interTFar[0] = interTFar[1] = 1000.0f;

		int objboxinter;
		//Intersect ray with BVH node
		bool hitobject[2];

		if ( maskedAt > S.curIndex  /*&& maskedAt!=-1*/ )
		{
			objboxinter = GvRendering::intersectBox( rayStart, rayDir, Nlr[0].bbMin(), Nlr[0].bbMax(), interTNear[0], interTFar[0]);
			hitobject[0] = objboxinter && interTFar[0] > 0.0f;
			//interTNear[0] = maxcc( interTNear[0], 0.0f );
			
			objboxinter = GvRendering::intersectBox( rayStart, rayDir, Nlr[1].bbMin(), Nlr[1].bbMax(), interTNear[1], interTFar[1]);
			hitobject[1] = objboxinter && interTFar[1] > 0.0f;
			//interTNear[1] = maxcc( interTNear[1], 0.0f );
		}
		else
		{
			hitobject[0] = false;
			hitobject[1] = false;
		}

		///////////////////////
		uint b1, b2;
#if 1 //BVH_TRAVERSAL_USE_MASKSTACK
		b1 = (uint)(hitobject[0] && interTNear[0]<=t );
		b2 = (uint)(hitobject[1] && interTNear[1]<=t );
#else
		b1 =(uint)(hitobject[0]);
		b2 =(uint)(hitobject[1]);
#endif
		///////////////////////
#if BVH_TRAVERSAL_USE_MASKSTACK
		__shared__ uint blockMask[2];
#if 0
		uint maskVal[2];
		maskVal[0]=b1<<Pid;
		maskVal[1]=b2<<Pid;

		atomicOr(blockMask, maskVal[0]);
		atomicOr(blockMask+1, maskVal[1]);
#else
		__shared__ uint blockMaskList[NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y];
		blockMaskList[Pid]=b1<<Pid;
		groupParallelOR(Pid, blockMaskList, NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y);
		__syncthreads();

		if(Pid==0)
			blockMask[0]=blockMaskList[0];
		__syncthreads();

		blockMaskList[Pid]=b2<<Pid;
		groupParallelOR(Pid, blockMaskList, NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y);
		__syncthreads();

		if(Pid==0)
			blockMask[1]=blockMaskList[0];

#endif
		__syncthreads();
#endif

		///////////////////////
		if ( Pid < 4 )
		{
			sharedHits[Pid] = 0;
		}
		__syncthreads();

		sharedHits[ 2 * b1 + b2 ] = true;
		__syncthreads();

		if ( sharedHits[3] || (sharedHits[1] && sharedHits[2]) )
		{
			float b2state = ( b2 && interTNear[1] < interTNear[0] ) ? 1.0f : 0.0f;

			if ( !b1 && !b2 )
			{
				b2state = 0.5f;
			}

			M[Pid] = 2.0f * b2state - 1.0f;

			__syncthreads();

			groupParallelSum( Pid, M, NUM_RAYS_PER_BLOCK_X * NUM_RAYS_PER_BLOCK_Y );
			__syncthreads();
			
			if ( M[0] < 0.0f )
			{
				if ( Pid == 0 )
				{
					S.push( snode.getSubNodeIdx() + 1 );
					nodeStack.push( Nlr[1] );
#if BVH_TRAVERSAL_USE_MASKSTACK
					nodeMaskStack.push(blockMask[1]);
#endif
				}
				__syncthreads();
				
				if ( Pid == 0 )
				{
					Np = snode.getSubNodeIdx();
					snode = Nlr[0];
				}
				//__syncthreads();

				lastInterT = interTNear[0];
				lastInterTmax = interTFar[0];

				if ( maskedAt > S.curIndex && !b1 )
				{
					maskedAt = S.curIndex;
				}
			}
			else
			{
				if ( Pid == 0 )
				{
					S.push( snode.getSubNodeIdx() );
					nodeStack.push( Nlr[0] );
#if BVH_TRAVERSAL_USE_MASKSTACK
					nodeMaskStack.push(blockMask[0]);
#endif
				}
				__syncthreads();

				if ( Pid == 0 )
				{
					Np = snode.getSubNodeIdx() + 1;
					snode = Nlr[1];
				}
				//__syncthreads();

				lastInterT = interTNear[1];
				lastInterTmax = interTFar[1];

				if ( maskedAt > S.curIndex && !b2 )
				{
					maskedAt=S.curIndex;
				}
			}

			//__syncthreads();
		}
		else
		{
			if ( sharedHits[2] ){ //Warning, M[2] and M[1] cases inverted in the original paper algorithm
				if ( maskedAt > S.curIndex && !b1 )
				{
					maskedAt = S.curIndex;
				}

				if ( Pid == 0 )
				{
					Np = snode.getSubNodeIdx();
					snode = Nlr[0];
				}
				//__syncthreads();

				lastInterT = interTNear[0];
				lastInterTmax = interTFar[0];
			}
			else if ( sharedHits[1] )
			{
				if ( maskedAt > S.curIndex && !b2 )
				{
					maskedAt = S.curIndex;
				}

				if ( Pid == 0 )
				{
					Np = snode.getSubNodeIdx() + 1;
					snode = Nlr[1];
				}
				//__syncthreads();

				lastInterT = interTNear[1];
				lastInterTmax = interTFar[1];
			}
			else
			{
				if ( S.isEmpty() )
				{
					maskedAt = -1;
				}
				else
				{
					//Pop required
					Np = 0;
				}
				//__syncthreads();
			}
		}

		__syncthreads();
		
		if ( Pid == 0 )
		{
			stopTraversal = true;
		}
		__syncthreads();

		if ( ( maskedAt != -1 ) && ( ( Np == 0 ) || snode.hasSubNodes() ) )
		{
			stopTraversal = false;
		}
		__syncthreads();
	}
}

/******************************************************************************
 * initStacks ...
 *
 * @param Pid global index of one element of a rendering tile (i.e. pixel)
 ******************************************************************************/
__device__
void initStacks( uint Pid )
{
	// Init shared stacks
	S.init( Pid );
	nodeStack.init( Pid );
#if BVH_TRAVERSAL_USE_MASKSTACK
	nodeMaskStack.init(Pid);
#endif

	// Shared memory
	__shared__ VolTreeBVHNodeUser rootNode;
	if ( Pid == 0 )
	{
		rootNode.bbox.pMin = make_float3( 0.0f );
		rootNode.bbox.pMax = make_float3( 1.0f );
		rootNode.setSubNodeIdx( 2 );
		rootNode.setGPULink();
	}

#if BVH_TRAVERSAL_USE_MASKSTACK
	__shared__ uint blockMask;
	uint maskVal;
	maskVal=uint(!masked)<<Pid;
	atomicOr(&blockMask, maskVal);

	__syncthreads();
	if(Pid==0){
		nodeMaskStack.push(blockMask);
	}
#endif

	if ( Pid == 0 )
	{
		S.push( 0 );
		nodeStack.push( rootNode );
	}
	__syncthreads();
}


//EscapeIdx traversal

template<class GPUTreeBVHType>
__device__
void d_bvhTraversalStep_SharedEscape(GPUTreeBVHType &gpuTreeBVH, 
									 uint &Pid, int &maskedAt, 
									 uint2 &tileCoords, 
									 float3 &rayStart, float3 &rayDir, 
									 uint &Np, VolTreeBVHNodeUser &snode, 
									 float &t, float &lastInterT, float &lastInterTmax){

}

template<class GPUTreeBVHType>
__device__
inline void d_bvhBBoxInterStep_SharedEscape(GPUTreeBVHType &gpuTreeBVH, 
									 uint &Pid,
									 float3 &bboxMin, float3 &bboxMax, 
									 uint &Np, VolTreeBVHNodeUser &snode){

	float3 testSphereCenter;
	float testShereRadius;


	testSphereCenter=(bboxMin+bboxMax)/make_float3(2.0f);

	float3 bboxSize=bboxMax-bboxMin;
	testShereRadius=max(bboxSize.x, max(bboxSize.y, bboxSize.z))/2.0f;

	do{

		//gpuTreeBVH.parallelFetchBVHNode(Pid, snode, Np);
		gpuTreeBVH.fetchBVHNode(snode, Np);
		

		__syncthreads();

		__shared__ bool inter;

		inter=sphereBoxIntersect(testSphereCenter, testShereRadius, snode.bbox.pMin, snode.bbox.pMax);

		__syncthreads();

		if(inter && snode.isLinkActive()){

			if(snode.hasSubNodes()){
				Np=snode.getSubNodeIdx();
			}else{
				return;
			}

		}else{
			Np=snode.escapeIdx;
		}

		__syncthreads();

	}while(Np);
}



template<class GPUTreeBVHType>
__device__
inline void d_bvhCreateInterDataList(GPUTreeBVHType &gpuTreeBVH, 
									 uint &Pid,
									 float3 &bboxMin, float3 &bboxMax, 
									 uint *sharedDataIdxList, uint maxNumElem,
									 uint &numElem) 
{

	numElem=0;

	__shared__ uint Np;
	__shared__ VolTreeBVHNodeUser snode;

	Np=2;

	do{

		d_bvhBBoxInterStep_SharedEscape(gpuTreeBVH, Pid, bboxMin, bboxMax, Np, snode);

		//gpuTreeBVH.parallelFetchBVHNode(Pid, snode, Np);

		__syncthreads();


		if(Np){
			if( snode.isDataType() && snode.isLinkActive() ){

				if(Pid==0){
					sharedDataIdxList[numElem]=snode.getDataIdx();

					numElem++;

					
				}

			}
			
			Np=snode.escapeIdx;

		}

		__syncthreads();

	}while(Np && numElem<maxNumElem);
}

#endif

////BVHTrianglesManip.hcu
//#ifndef BVHTrianglesManip_hcu
//#define BVHTrianglesManip_hcu
//
///******************************************************************************
// ******************************* INCLUDE SECTION ******************************
// ******************************************************************************/
//
//// Gigavoxels
//#include <GvCore/IntersectionTests.hcu>
//#include <GvCore/RendererHelpers.hcu>
//
////#include "BvhTree.hcu"
////#include "IntersectionTests.h"
//
//__device__
//void groupParallelSum(uint Tid, int *data, uint n){
//	for(uint stride=n>>1; stride>0; stride=stride>>1){
//		__syncthreads();
//		if(Tid<stride){
//			data[Tid]+=data[Tid+stride];
//		}
//
//	}
//}
//
//__device__
//void groupParallelSum(uint Tid, float *data, uint n){
//	for(uint stride=n>>1; stride>0; stride=stride>>1){
//		__syncthreads();
//		if(Tid<stride){
//			data[Tid]+=data[Tid+stride];
//		}
//	}
//}
//
//__device__
//void groupParallelMin(uint Tid, float *data, uint n){
//	for(uint stride=n>>1; stride>0; stride=stride>>1){
//		if(Tid<stride){
//			data[Tid] = data[Tid] <= data[Tid+stride] ? data[Tid] : data[Tid+stride];
//			//data[Tid]=fminf(data[Tid], data[Tid+stride]);
//		}
//		__syncthreads();
//	}
//}
//
//
//__device__
//inline void groupParallelOR(uint Tid, uint *data, uint n){
//	for(uint stride=n>>1; stride>0; stride=stride>>1){
//		__syncthreads();
//		if(Tid<stride){
//			data[Tid]=data[Tid] | data[Tid+stride];
//		}
//
//	}
//}
//
///** BVH traversal using shared stack. 'Realtime Ray Tracing on GPU with BVH-based Packet Traversal' */
//template<class T>
//class TraversalStack{
//public:
//	int curIndex;
//	T data[BVH_TRAVERSAL_STACK_SIZE];
//
//	__device__
//	void init(uint Pid){
//		if(Pid==0)
//			curIndex=0;
//		__syncthreads();
//	}
//
//	__device__
//	inline T &top(){
//		return data[curIndex];
//	}
//
//
//	__device__
//	inline T pop(uint Pid){
//		if(Pid==0		&& curIndex>0)
//			curIndex--;
//		//__syncthreads();
//		return data[curIndex];
//	}
//
//	__device__
//	inline T pop(){
//		//if(curIndex>0)
//			curIndex--;
//		return data[curIndex];
//	}
//
//	__device__
//	inline void push(uint Pid, const T &val){
//		if(Pid==0){
//			if(curIndex<BVH_TRAVERSAL_STACK_SIZE	  && curIndex>=0){
//				data[curIndex]=val;
//				curIndex++;
//			}
//		}
//	}
//	__device__
//	inline void push(const T &val){
//		//if(curIndex<BVH_TRAVERSAL_STACK_SIZE		&& curIndex>=0){
//			data[curIndex]=val;
//			curIndex++;
//		//}
//	}
//
//	__device__
//	inline bool isEmpty(){
//		//__syncthreads();
//		return (curIndex<=0);
//	}
//	__device__
//	inline bool isFull(){
//		//__syncthreads();
//		return (curIndex>=BVH_TRAVERSAL_STACK_SIZE);
//	}
//
//};
//
//
//template<class T, uint Size>
//struct CudaQueue{
//
//	T data[Size];
//
//	__device__
//	inline void clear(T v){
//		switch(Size){
//		case 8:
//			data[7]=v;
//		case 7:
//			data[6]=v;
//		case 6:
//			data[5]=v;
//		case 5:
//			data[4]=v;
//		case 4:
//			data[3]=v;
//		case 3:
//			data[2]=v;
//		case 2:
//			data[1]=v;
//		case 1:
//			data[0]=v;
//		}
//	}
//
//
//	__device__
//	inline T &top(){
//		return data[0];
//	}
//
//	__device__
//	inline void push(const T &val){
//		switch(Size){
//		case 8:
//			data[7]=data[6];
//		case 7:
//			data[6]=data[5];
//		case 6:
//			data[5]=data[4];
//		case 5:
//			data[4]=data[3];
//		case 4:
//			data[3]=data[2];
//		case 3:
//			data[2]=data[1];
//		case 2:
//			data[1]=data[0];
//		}
//
//		data[0]=val;
//	}
//};
//
//
//
//__shared__ TraversalStack<uint> S;
//__shared__ TraversalStack<VolTreeBVHNodeUser> nodeStack;
//
//#define STACKALL_MODE 0
//
//template<class GPUTreeBVHType>
//__device__
//void d_bvhTraversalStep_SharedStack(GPUTreeBVHType &gpuTreeBVH, uint &Pid, int &maskedAt, uint2 &tileCoords, float3 &rayStart, float3 &rayDir, uint &Np, VolTreeBVHNodeUser &snode, float &t){
//
//	//__shared__ VolTreeBVHNodeUser snode;
//	__shared__ bool stopTraversal;
//
//
//	if(S.isEmpty()){
//		maskedAt=-1;
//	}
//
//	if(Pid==0){
//		Np=S.pop();
//		snode=nodeStack.pop();
//	}
//	__syncthreads();
//
//
//	if(Pid==0){
//		stopTraversal=true;
//	}
//	__syncthreads();
//
//	if(maskedAt!=-1 && snode.hasSubNodes()){
//		stopTraversal=false;
//	}
//	__syncthreads();
//
//
//
//	while( !stopTraversal ){
//
//		if(maskedAt>S.curIndex ){
//			maskedAt=BVH_TRAVERSAL_STACK_SIZE;
//
//			//Intersect ray with BVH node
//			float interMin; float interMax;
//			int objboxinter=intersectBox( rayStart, rayDir,  snode.bbMin(), snode.bbMax(), interMin, interMax);
//			bool snodeInter= (objboxinter && interMax>0.0f);
//			interMin=maxcc(interMin, 0.0f);
//
//			if(!snodeInter){
//				maskedAt=S.curIndex;
//			}else if(interMin>t){
//				maskedAt=S.curIndex;
//			}
//		}
//		__syncthreads();
//
//
//		__shared__ VolTreeBVHNodeUser Nlr[2];
//		__shared__ uint sharedHits[4];
//		__shared__ float M[NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y ]; //BVH_NUM_THREADS_PER_BLOCK_X*BVH_NUM_THREADS_PER_BLOCK_Y
//
//		uint b1, b2;
//		float lambda1; float lambda2;
//		float mu1; float mu2;
//
//		//Parallel fetch of the 2 childs
//		gpuTreeBVH.parallelFetchBVHNode(Pid, Nlr[0], snode.getSubNodeIdx());
//		//__syncthreads();
//		gpuTreeBVH.parallelFetchBVHNode(Pid, Nlr[1], snode.getSubNodeIdx()+1);
//		//__syncthreads();
//
//		int objboxinter;
//		//Intersect ray with BVH node
//		bool hitobjectLambda;
//		bool hitobjectMu;
//
//		if(maskedAt>S.curIndex  /*&& maskedAt!=-1*/){
//			objboxinter=intersectBox( rayStart, rayDir,  Nlr[0].bbMin(), Nlr[0].bbMax(), lambda1, lambda2);
//			hitobjectLambda=objboxinter && lambda2>0.0f;
//			lambda1=maxcc(lambda1, 0.0f);
//
//
//			objboxinter=intersectBox( rayStart, rayDir,  Nlr[1].bbMin(), Nlr[1].bbMax(), mu1, mu2);
//			hitobjectMu=objboxinter && mu2>0.0f;
//			mu1=maxcc(mu1, 0.0f);
//
//		}else{
//			hitobjectLambda=false;
//			hitobjectMu=false;
//		}
//
//
//		b1 =(uint)(hitobjectLambda );
//		b2 =(uint)(hitobjectMu );
//
//		if(Pid<4)
//			sharedHits[Pid]=0;
//		__syncthreads();
//
//		sharedHits[2*b1+b2]=true;
//		__syncthreads();
//
//		if(  sharedHits[3] || (sharedHits[1] && sharedHits[2]) ){
//
//			/*if(Pid==0)
//				M[0]=-1;*/
//			float b2state= (b2 && mu1<lambda1) ? 1.0f : 0.0f;
//
//			if(!b1 && !b2)
//				b2state=0.5f;
//
//			M[Pid]=2.0f* b2state -1.0f;
//
//			__syncthreads();
//
//
//			groupParallelSum( Pid, M, NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y /*BVH_NUM_THREADS_PER_BLOCK_X*BVH_NUM_THREADS_PER_BLOCK_Y*/);
//			/*if(Pid==0){
//				float summ=0.0f;
//
//				for(uint ii=0; ii<NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y; ii++){
//					summ+=M[ii];
//				}
//				M[0]=summ;
//			}*/
//			__syncthreads();
//
//
//			if(M[0]<0.0f){
//
//				/*if(maskedAt>S.curIndex && !b2)
//					maskedAt=S.curIndex;*/
//				if(Pid==0){
//					S.push(snode.getSubNodeIdx()+1);
//					nodeStack.push(Nlr[1]);
//				}
//				__syncthreads();
//
//				/*if(maskedAt>S.curIndex && !b1)
//					maskedAt=S.curIndex;*/
//				if(Pid==0){
//#if STACKALL_MODE
//					S.push(snode.getSubNodeIdx());
//					nodeStack.push(Nlr[0]);
//#else
//					Np=snode.getSubNodeIdx();
//					snode=Nlr[0];
//#endif
//				}
//				//__syncthreads();
//
//
//			}else{
//				/*if(maskedAt>S.curIndex && !b1)
//					maskedAt=S.curIndex;*/
//				if(Pid==0){
//					S.push(snode.getSubNodeIdx());
//					nodeStack.push(Nlr[0]);
//				}
//				__syncthreads();
//				/*if(maskedAt>S.curIndex && !b2)
//					maskedAt=S.curIndex;*/
//				if(Pid==0){
//#if STACKALL_MODE
//					S.push(snode.getSubNodeIdx()+1);
//					nodeStack.push(Nlr[1]);
//#else
//					Np=snode.getSubNodeIdx()+1;
//					snode=Nlr[1];
//#endif
//				}
//				//__syncthreads();
//			}
//
//			//__syncthreads();
//
//		}else{
//
//			if(sharedHits[2]){ //Warning, M[2] and M[1] cases inverted in the original paper algorithm
//				if(maskedAt>S.curIndex && !b1		  /*&& maskedAt!=-1*/)
//						maskedAt=S.curIndex;
//
//				if(Pid==0){
//#if STACKALL_MODE
//					S.push(snode.getSubNodeIdx());
//					nodeStack.push(Nlr[0]);
//#else
//					Np=snode.getSubNodeIdx();
//					snode=Nlr[0];
//#endif
//				}
//				//__syncthreads();
//			}else if(sharedHits[1]){
//
//				if(maskedAt>S.curIndex && !b2	 /*&& maskedAt!=-1*/)
//					maskedAt=S.curIndex;
//
//				if(Pid==0){
//#if STACKALL_MODE
//					S.push(snode.getSubNodeIdx()+1);
//					nodeStack.push(Nlr[1]);
//#else
//					Np=snode.getSubNodeIdx()+1;
//					snode=Nlr[1];
//#endif
//				}
//				//__syncthreads();
//			}
//#if STACKALL_MODE==0
//			else{
//
//				if(S.isEmpty()){
//					maskedAt=-1;
//				}else{
//					if(Pid==0){
//						Np=S.pop();
//						snode=nodeStack.pop();
//					}
//				}
//				//__syncthreads();
//			}
//#endif
//
//		}
//
//		__syncthreads();
//
//#if STACKALL_MODE
//		if(S.isEmpty()){
//			maskedAt=-1;
//		}
//
//		__syncthreads();
//		if(Pid==0){
//			Np=S.pop();
//			snode=nodeStack.pop();
//		}
//		__syncthreads();
//#endif
//		if(Pid==0)
//			stopTraversal=true;
//		__syncthreads();
//
//		if( maskedAt!=-1 && snode.hasSubNodes() ){
//			stopTraversal=false;
//		}
//		__syncthreads();
//
//	}
//}
//
//
//#if BVH_TRAVERSAL_USE_MASKSTACK
//__shared__ TraversalStack<uint> nodeMaskStack;
//#endif
//
//template < typename BvhTreeKernelType >
//__device__
//void d_bvhTraversalStep_SharedStack2(BvhTreeKernelType &gpuTreeBVH, const uint Pid, int &maskedAt, const float3 rayStart, const float3 rayDir,
//	uint &Np, VolTreeBVHNodeUser &snode, float &t, float &lastInterT, float &lastInterTmax)
//{
//	//__shared__ VolTreeBVHNodeUser snode;
//	__shared__ bool stopTraversal;
//
//	stopTraversal=false;
//
//	while (!stopTraversal)
//	{
//		if (Np == 0)
//		{
//			// Pop the current node from the stack
//			if (Pid == 0)
//			{
//				Np = S.pop();
//				snode = nodeStack.pop();
//			}
//
//			__syncthreads();
//
//			if (maskedAt > S.curIndex)
//			{
//				//Intersect ray with BVH node
//				float interMin=0.0f; float interMax=1000.0f;
//				int objboxinter=intersectBox( rayStart, rayDir,  snode.bbMin(), snode.bbMax(), interMin, interMax);
//				bool snodeInter= (objboxinter && interMax>0.0f);
//				//interMin=maxcc(interMin, 0.0f);
//
//				lastInterT=interMin;
//				lastInterTmax=interMax;
//
//				// FIXME:
//				float pixelSize = k_renderViewContext.pixelSize.x * interMin * 1.333f * k_renderViewContext.frustumNearINV;
//				//float pixelSize=getPixelSizeAtDist<0>(interMin)*1.0f;
//
//				if (!snodeInter || interMin>t || snode.bbox.maxSize() < pixelSize) {
//					maskedAt = S.curIndex;
//				} else {
//					maskedAt = BVH_TRAVERSAL_STACK_SIZE;
//				}
//			}
//
//			if (!snode.hasSubNodes())
//				break;
//
//		} //if(Np==0)
//
//		__shared__ VolTreeBVHNodeUser Nlr[2];
//		__shared__ uint sharedHits[4];
//		__shared__ float M[NUM_RAYS_PER_BLOCK_X*NUM_RAYS_PER_BLOCK_Y];
//
//		//Parallel fetch of the 2 childs
//		gpuTreeBVH.parallelFetchBVHNodeTile(Pid, Nlr, snode.getSubNodeIdx());
//
//		float interTNear[2]; float interTFar[2];
//		interTNear[0]=interTNear[1]=0.0f; interTFar[0]=interTFar[1]=1000.0f;
//
//		int objboxinter;
//		//Intersect ray with BVH node
//		bool hitobject[2];
//
//		if (maskedAt > S.curIndex/*&& maskedAt!=-1*/)
//		{
//			objboxinter = intersectBox(rayStart, rayDir,  Nlr[0].bbMin(), Nlr[0].bbMax(), interTNear[0], interTFar[0]);
//			hitobject[0] = objboxinter && interTFar[0]>0.0f;
//			//interTNear[0]=maxcc(interTNear[0], 0.0f);
//
//			objboxinter = intersectBox(rayStart, rayDir,  Nlr[1].bbMin(), Nlr[1].bbMax(), interTNear[1], interTFar[1]);
//			hitobject[1] = objboxinter && interTFar[1]>0.0f;
//			//interTNear[1]=maxcc(interTNear[1], 0.0f);
//		}
//		else
//		{
//			hitobject[0] = false;
//			hitobject[1] = false;
//		}
//
//		///////////////////////
//		uint b1, b2;
//
//		b1 =(uint)(hitobject[0] && interTNear[0] <= t);
//		b2 =(uint)(hitobject[1] && interTNear[1] <= t);
//
//		///////////////////////
//		if(Pid<4)
//			sharedHits[Pid]=0;
//		__syncthreads();
//
//		sharedHits[2*b1+b2]=true;
//		__syncthreads();
//
//		if (sharedHits[3] || (sharedHits[1] && sharedHits[2]))
//		{
//			float b2state= (b2 && interTNear[1]<interTNear[0]) ? 1.0f : 0.0f;
//
//			if (!b1 && !b2)
//				b2state=0.5f;
//
//			M[Pid] = 2.0f * b2state -1.0f;
//			__syncthreads();
//
//			groupParallelSum(Pid, M, NUM_RAYS_PER_BLOCK_X * NUM_RAYS_PER_BLOCK_Y);
//			__syncthreads();
//
//			if (M[0] < 0.0f)
//			{
//				if (Pid == 0)
//				{
//					S.push(snode.getSubNodeIdx() + 1);
//					nodeStack.push(Nlr[1]);
//				}
//				__syncthreads();
//
//
//				if (Pid == 0)
//				{
//					Np=snode.getSubNodeIdx();
//					snode=Nlr[0];
//				}
//				//__syncthreads();
//
//				lastInterT=interTNear[0];
//				lastInterTmax=interTFar[0];
//
//				if (maskedAt > S.curIndex && !b1)
//					maskedAt = S.curIndex;
//			}
//			else
//			{
//				if (Pid == 0)
//				{
//					S.push(snode.getSubNodeIdx());
//					nodeStack.push(Nlr[0]);
//				}
//				__syncthreads();
//
//				if (Pid == 0)
//				{
//					Np=snode.getSubNodeIdx()+1;
//					snode=Nlr[1];
//				}
//				//__syncthreads();
//
//				lastInterT=interTNear[1];
//				lastInterTmax=interTFar[1];
//
//				if (maskedAt > S.curIndex && !b2)
//					maskedAt = S.curIndex;
//			}
//			//__syncthreads();
//		}
//		else
//		{
//			if (sharedHits[2]) //Warning, M[2] and M[1] cases inverted in the original paper algorithm
//			{
//				if (maskedAt > S.curIndex && !b1)
//					maskedAt = S.curIndex;
//
//				if (Pid == 0)
//				{
//					Np=snode.getSubNodeIdx();
//					snode=Nlr[0];
//				}
//				//__syncthreads();
//
//				lastInterT=interTNear[0];
//				lastInterTmax=interTFar[0];
//			}
//			else if(sharedHits[1])
//			{
//				if (maskedAt > S.curIndex && !b2)
//					maskedAt = S.curIndex;
//
//				if (Pid == 0)
//				{
//					Np=snode.getSubNodeIdx()+1;
//					snode=Nlr[1];
//				}
//				//__syncthreads();
//
//				lastInterT=interTNear[1];
//				lastInterTmax=interTFar[1];
//			}
//			else
//			{
//				if (S.isEmpty())
//				{
//					maskedAt=-1;
//				}
//				else
//				{
//					//Pop required
//					Np=0;
//				}
//				//__syncthreads();
//			}
//
//		}
//
//		__syncthreads();
//
//		if (Pid == 0)
//			stopTraversal = true;
//		__syncthreads();
//
//		if (maskedAt != -1 && (Np==0 || snode.hasSubNodes())) {
//			stopTraversal=false;
//		}
//		__syncthreads();
//	}
//}
//
//__device__
//void initStacks(uint Pid)
//{
//	//Init shared stack
//	S.init(Pid);
//	nodeStack.init(Pid);
//
//	__shared__ VolTreeBVHNodeUser rootNode;
//
//	if(Pid==0)
//	{
//		rootNode.bbox.pMin=make_float3(0.0f);
//		rootNode.bbox.pMax=make_float3(1.0f);
//		rootNode.setSubNodeIdx(2);
//		rootNode.setGPULink();
//	}
//
//	if(Pid==0)
//	{
//		S.push(0);
//		nodeStack.push(rootNode);
//	}
//
//	__syncthreads();
//}
//
////EscapeIdx traversal
//
////template<class GPUTreeBVHType>
////__device__
////void d_bvhTraversalStep_SharedEscape(GPUTreeBVHType &gpuTreeBVH, 
////									 uint &Pid, int &maskedAt, 
////									 uint2 &tileCoords, 
////									 float3 &rayStart, float3 &rayDir, 
////									 uint &Np, VolTreeBVHNodeUser &snode, 
////									 float &t, float &lastInterT, float &lastInterTmax){
////
////}
////
////template<class GPUTreeBVHType>
////__device__
////inline void d_bvhBBoxInterStep_SharedEscape(GPUTreeBVHType &gpuTreeBVH, 
////									 uint &Pid,
////									 float3 &bboxMin, float3 &bboxMax, 
////									 uint &Np, VolTreeBVHNodeUser &snode){
////
////	float3 testSphereCenter;
////	float testShereRadius;
////
////
////	testSphereCenter=(bboxMin+bboxMax)/make_float3(2.0f);
////
////	float3 bboxSize=bboxMax-bboxMin;
////	testShereRadius=max(bboxSize.x, max(bboxSize.y, bboxSize.z))/2.0f;
////
////	do{
////
////		//gpuTreeBVH.parallelFetchBVHNode(Pid, snode, Np);
////		gpuTreeBVH.fetchBVHNode(snode, Np);
////		
////
////		__syncthreads();
////
////		__shared__ bool inter;
////
////		inter=sphereBoxIntersect(testSphereCenter, testShereRadius, snode.bbox.pMin, snode.bbox.pMax);
////
////		__syncthreads();
////
////		if(inter && snode.isLinkActive()){
////
////			if(snode.hasSubNodes()){
////				Np=snode.getSubNodeIdx();
////			}else{
////				return;
////			}
////
////		}else{
////			Np=snode.escapeIdx;
////		}
////
////		__syncthreads();
////
////	}while(Np);
////}
//
//
//
////template<class GPUTreeBVHType>
////__device__
////inline void d_bvhCreateInterDataList(GPUTreeBVHType &gpuTreeBVH, 
////									 uint &Pid,
////									 float3 &bboxMin, float3 &bboxMax, 
////									 uint *sharedDataIdxList, uint maxNumElem,
////									 uint &numElem) 
////{
////
////	numElem=0;
////
////	__shared__ uint Np;
////	__shared__ VolTreeBVHNodeUser snode;
////
////	Np=2;
////
////	do{
////
////		d_bvhBBoxInterStep_SharedEscape(gpuTreeBVH, Pid, bboxMin, bboxMax, Np, snode);
////
////		//gpuTreeBVH.parallelFetchBVHNode(Pid, snode, Np);
////
////		__syncthreads();
////
////
////		if(Np){
////			if( snode.isDataType() && snode.isLinkActive() ){
////
////				if(Pid==0){
////					sharedDataIdxList[numElem]=snode.getDataIdx();
////
////					numElem++;
////
////					
////				}
////
////			}
////			
////			Np=snode.escapeIdx;
////
////		}
////
////		__syncthreads();
////
////	}while(Np && numElem<maxNumElem);
////}
//
//#endif
